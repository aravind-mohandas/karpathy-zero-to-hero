# Neural Networks: Zero to Hero ğŸš€

This repository contains my implementations and notes from Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* lecture series.  
The goal is to deeply understand how modern neural networks work â€” by building everything from scratch, step by step.

---

## ğŸ“š Overview

The series covers the full journey from the foundations of automatic differentiation to building a GPT-style transformer model.

| Module | Description |
|--------|--------------|
| **micrograd/** | A tiny autograd engine and a small neural net library built from scratch. |
| **makemore/** | Character-level language models (bigram, MLP, and simple neural nets). |
| **minGPT/** | A minimal GPT implementation with attention, layer normalization, and training loops. |
| **notes.md** | My takeaways and key concepts from each lecture. |

---

## ğŸ§  Learning Goals
- Strengthen intuition for how backpropagation and gradient descent really work  
- Understand neural network design from first principles  
- Write core ML components (forward, backward, training) without frameworks  
- Connect these fundamentals to how modern LLMs like GPT are built

---

## ğŸ› ï¸ Tech Stack
- Python  
- PyTorch (for parts where abstraction helps)  
- NumPy  
- Jupyter Notebooks  

---

## ğŸ™Œ Acknowledgments
All credits to **Andrej Karpathy** for the amazing *Zero to Hero* series.  
Iâ€™m simply reimplementing and documenting my learning journey here.

---

> â€œWhat I cannot create, I do not understand.â€ â€” Richard Feynman
