{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b954be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the input\n",
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033c9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# characters 1115393\n"
     ]
    }
   ],
   "source": [
    "print(\"# characters\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ddca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# unique characers that occur\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb30987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43, 1, 2]\n",
      "Hi there !\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join(itos[i] for i in l )\n",
    "\n",
    "print(encode(\"hii there !\"))\n",
    "print(decode(encode(\"Hi there !\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "196efa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it in a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcc085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into train and val sets\n",
    "n  = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ee1db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec450c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input in tensor([18]) the target: 47\n",
      "when input in tensor([18, 47]) the target: 56\n",
      "when input in tensor([18, 47, 56]) the target: 57\n",
      "when input in tensor([18, 47, 56, 57]) the target: 58\n",
      "when input in tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input in tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input in tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input in tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# one single block has <block_size> examples\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input in {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c663f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      " torch.Size([4, 8]) tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "outputs:\n",
      " torch.Size([4, 8]) tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
      "---\n",
      "when input is [53] the target: 59\n",
      "when input is [53, 59] the target: 6\n",
      "when input is [53, 59, 6] the target: 1\n",
      "when input is [53, 59, 6, 1] the target: 58\n",
      "when input is [53, 59, 6, 1, 58] the target: 56\n",
      "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
      "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
      "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel\n",
    "block_size = 8  # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i + block_size + 1] for i in ix ])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:\\n', xb.shape, xb)\n",
    "print('outputs:\\n', yb.shape, yb)\n",
    "\n",
    "print('---')\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b129563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.894842624664307\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# bigram model\n",
    "# direct pytorch implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers \n",
    "        logits = self.token_embedding_table(idx) # (batch size, time(block_size), channels)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus on the last time step only\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1,1),dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1cc5a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.666220188140869\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none= True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a455036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1),dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2174713",
   "metadata": {},
   "source": [
    "The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d9e4bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4014e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words - used by people when averaging up things\n",
    "# inital approach to aggregating information - average the information for all the characters preceeding the current character\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca50cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is super inefficient, we use matrix multiplication to fix this\n",
    "# we can row-normalised traingular matrices for this neat trick\n",
    "wei = torch.tril(torch.ones (T, T)) # wei is short for weights\n",
    "wei = wei/wei.sum(1, keepdim = True)\n",
    "xbow2 = wei @ x # (B(added), T, T) @ (B, T, C) ----> (B, T, C) . parallel multiplication along the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2803eae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: using softmax instead of tril\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril ==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = 1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "\n",
    "# its exactly the same, but it is more intuitive when we set the values to -inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8faf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention ! We made it\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# lets see a single head perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ----> (B, T, T)\n",
    "\n",
    "\n",
    "# wei is now a function in a data dependent manner\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril ==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim = 1)\n",
    "\n",
    "v = value(x)    # (B, T, head_size)\n",
    "out = wei @ v    # (B, T, T) @ (B, T, head_size) ----> (B, T, head_size)\n",
    "\n",
    "# out = wei @ x\n",
    "\n",
    "out.shape\n",
    "\n",
    "# every single node(token) at each position emits two positions - query and key\n",
    "# query - what I am looking for\n",
    "# key - what do I contain\n",
    "# valu - what do I pass on\n",
    "# product of query and key gives us wei (or affinities)\n",
    "# The output shape changes because we multiply wei with value now\n",
    "# also makes sense why we use softmax - to normalise the weights\n",
    "# consider x to be private information to the token, v is the information that is aggregated and passed on\n",
    "\n",
    "\n",
    "# NOTES\n",
    "# 1. attention is a communication mechanism. Can be seen as nodes in a directed graph communicating with each other and passing information\n",
    "# 2. There is no notion of space. Attention acts over a set of vectors. This is why we add positional embeddings\n",
    "# 3. Each example accorss batch dimension is processed completely independently and never talk to each other\n",
    "# 4. in an encoder attention block, we can delete the line which does masking, allowing all tokens to communcate with each other freely.\n",
    "# 5. in a decoder attention block, we need the masking to prevent tokens from \"cheating\" by looking ahead.\n",
    "# 6. self attention means that the keys, queries and values all come from the same place (x in this case). In cross attention, queries come from one place, and keys and values come from another place (as is the case in an encoder decoder block, for example).\n",
    "# 7. scaled attention dvides the wei by sqrt(head_size) before applying softmax to prevent large dot products which lead to vanishing gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b9e1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization - a normalization strategy that normalizes the features within a single layer\n",
    "# we normalize the rows of the input rather than the columns\n",
    "\n",
    "\n",
    "\n",
    "class LayerNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps = 1e-51):\n",
    "        self.eps = eps\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim = True) # batch mean\n",
    "        xvar = x.var(1, keepdim = True) # batch variance\n",
    "    \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)       # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the buffers:\n",
    "        # if self.training:\n",
    "        #     with torch.no_grad():\n",
    "        #         self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        #         self.running_Var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        # since out computations doesnt span across examples we dont need to calculate this\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the original paper, add and norm are applied after every attention block and MLP block. \n",
    "# now, in practice it common, to apply the layer norm before transformation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-zero-to-hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
